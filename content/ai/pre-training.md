---
title: Pre-training
date: 2025-06-25
tags:
  - ai
  - training
  - foundation
---

The initial process of training language models on large amounts of unlabeled text. Autoregressive models (like Claude) are trained to predict the next word based on previous context.

Pre-training creates the foundational knowledge that enables language models to understand and generate human-like text across diverse domains and tasks.