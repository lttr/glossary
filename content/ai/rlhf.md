---
title: RLHF
date: 2025-06-25
tags:
  - ai
  - training
  - human-feedback
  - alignment
---

Reinforcement Learning from Human Feedback - a technique for training language models to behave in accordance with human preferences. Claude was trained using RLHF.

This approach helps align AI models with human values and preferences, making them more helpful, harmless, and honest in their interactions with users.